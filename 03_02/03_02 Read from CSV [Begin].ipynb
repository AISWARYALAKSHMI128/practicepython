{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Retention Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A manager at the bank is disturbed with more and more customers leaving their credit card services. They would really like to understand what characteristics lend themselves to someone who is going to churn so they can proactively go to the customer to provide them better services and turn customers' decisions in the opposite direction.\n",
    "\n",
    "- This dataset consists of 10,000 customers mentioning their age, salary, marital_status, credit card limit, credit card category, etc. There are nearly 18 features.\n",
    "\n",
    "- 16.07% of customers have churned.\n",
    "\n",
    "- [Dataset link](https://www.kaggle.com/datasets/whenamancodes/credit-card-customers-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read in a csv into python, we will be leveraging the Pandas library. Any package we want to use in Python will need an import statement. In addition to pandas which we will import using `import pandas as pd`, we will also import matplotlib and seaborn (libraries used for visualization) and numpy (a library for array manipulation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "sns.set()\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we will be using is `pd.read_csv` which implies we will be reading a comma separated value file. You can see this in the defaults for this method by typing `help(pd.read_csv)` and see that the separator is set to `,` with other helpful defaults like `header='infer'`. You can read through the rest to get familiar with parameters you can pass through that might be specific to what you may need and different from the defaults. \n",
    "\n",
    "If you type `pd.read` and then press `tab` you will see other methods available to you out of the box to read in files. Examples: `pd.read_excel`, `pd.read_pickle`, `pd.read_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step to read in a csv file is to know where the relative location is to your Python script. In this case, I've created a folder called `data/` that I will use to store any input data files. To read in the file, I will just pass the file name into the parenthesis and take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps I always do after reading in my file is to:\n",
    "\n",
    "1) `data.shape` to see the size of the dataset. The size will help me decide on how to manage working with the dataset if it happens to be large. Here we see this dataset has **10K+** rows of customer data and **23** columns describing the behavior of those customers.\n",
    "\n",
    "2) `data.head()` to see the top of the dataset and make any changes like renaming column names. The default will show the top 5 rows, but you can pass through any number you like (10,25, etc)\n",
    "\n",
    "3) `data.columns` to see what all the column names\n",
    "\n",
    "Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
